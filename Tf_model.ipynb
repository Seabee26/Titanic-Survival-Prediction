{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/titanic/train_data.csv\n/kaggle/input/titanic/test_data.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/titanic/train_data.csv')\ndf_t = pd.read_csv('/kaggle/input/titanic/test_data.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df\n","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"     Unnamed: 0  PassengerId  Survived  Sex     Age      Fare  Pclass_1  \\\n0             0            1         0    1  0.2750  0.014151         0   \n1             1            2         1    0  0.4750  0.139136         1   \n2             2            3         1    0  0.3250  0.015469         0   \n3             3            4         1    0  0.4375  0.103644         1   \n4             4            5         0    1  0.4375  0.015713         0   \n..          ...          ...       ...  ...     ...       ...       ...   \n787         787          788         0    1  0.1000  0.056848         0   \n788         788          789         1    1  0.0125  0.040160         0   \n789         789          790         0    1  0.5750  0.154588         1   \n790         790          791         0    1  0.3500  0.015127         0   \n791         791          792         0    1  0.2000  0.050749         0   \n\n     Pclass_2  Pclass_3  Family_size  Title_1  Title_2  Title_3  Title_4  \\\n0           0         1          0.1        1        0        0        0   \n1           0         0          0.1        1        0        0        0   \n2           0         1          0.0        0        0        0        1   \n3           0         0          0.1        1        0        0        0   \n4           0         1          0.0        1        0        0        0   \n..        ...       ...          ...      ...      ...      ...      ...   \n787         0         1          0.5        0        0        1        0   \n788         0         1          0.3        0        0        1        0   \n789         0         0          0.0        1        0        0        0   \n790         0         1          0.0        1        0        0        0   \n791         1         0          0.0        1        0        0        0   \n\n     Emb_1  Emb_2  Emb_3  \n0        0      0      1  \n1        1      0      0  \n2        0      0      1  \n3        0      0      1  \n4        0      0      1  \n..     ...    ...    ...  \n787      0      1      0  \n788      0      0      1  \n789      1      0      0  \n790      0      1      0  \n791      0      0      1  \n\n[792 rows x 17 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n      <th>Family_size</th>\n      <th>Title_1</th>\n      <th>Title_2</th>\n      <th>Title_3</th>\n      <th>Title_4</th>\n      <th>Emb_1</th>\n      <th>Emb_2</th>\n      <th>Emb_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.2750</td>\n      <td>0.014151</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.4750</td>\n      <td>0.139136</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.3250</td>\n      <td>0.015469</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>4</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.4375</td>\n      <td>0.103644</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>5</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.4375</td>\n      <td>0.015713</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>787</th>\n      <td>787</td>\n      <td>788</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.1000</td>\n      <td>0.056848</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>788</th>\n      <td>788</td>\n      <td>789</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0125</td>\n      <td>0.040160</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>789</th>\n      <td>789</td>\n      <td>790</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.5750</td>\n      <td>0.154588</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>790</th>\n      <td>790</td>\n      <td>791</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.3500</td>\n      <td>0.015127</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>791</th>\n      <td>791</td>\n      <td>792</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.2000</td>\n      <td>0.050749</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>792 rows Ã— 17 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"Index(['Unnamed: 0', 'PassengerId', 'Survived', 'Sex', 'Age', 'Fare',\n       'Pclass_1', 'Pclass_2', 'Pclass_3', 'Family_size', 'Title_1', 'Title_2',\n       'Title_3', 'Title_4', 'Emb_1', 'Emb_2', 'Emb_3'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['Unnamed: 0','PassengerId','Family_size','Title_1', 'Title_2',\n       'Title_3', 'Title_4', 'Emb_1', 'Emb_2', 'Emb_3'],inplace=True,axis=1)\ndf_t.drop(['Unnamed: 0','PassengerId','Family_size','Title_1', 'Title_2',\n       'Title_3', 'Title_4', 'Emb_1', 'Emb_2', 'Emb_3'],inplace=True,axis=1)\n","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"     Survived  Sex     Age      Fare  Pclass_1  Pclass_2  Pclass_3\n0           0    1  0.2750  0.014151         0         0         1\n1           1    0  0.4750  0.139136         1         0         0\n2           1    0  0.3250  0.015469         0         0         1\n3           1    0  0.4375  0.103644         1         0         0\n4           0    1  0.4375  0.015713         0         0         1\n..        ...  ...     ...       ...       ...       ...       ...\n787         0    1  0.1000  0.056848         0         0         1\n788         1    1  0.0125  0.040160         0         0         1\n789         0    1  0.5750  0.154588         1         0         0\n790         0    1  0.3500  0.015127         0         0         1\n791         0    1  0.2000  0.050749         0         1         0\n\n[792 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.2750</td>\n      <td>0.014151</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.4750</td>\n      <td>0.139136</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.3250</td>\n      <td>0.015469</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0.4375</td>\n      <td>0.103644</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.4375</td>\n      <td>0.015713</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>787</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.1000</td>\n      <td>0.056848</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>788</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0125</td>\n      <td>0.040160</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>789</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.5750</td>\n      <td>0.154588</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>790</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.3500</td>\n      <td>0.015127</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>791</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.2000</td>\n      <td>0.050749</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>792 rows Ã— 7 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X=df.copy();\nX.drop('Survived',inplace=True,axis=1)\ny=df['Survived']\nX_t=df_t.copy();\nX_t.drop('Survived',inplace=True,axis=1)\ny_t=df_t['Survived']","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"     Sex     Age      Fare  Pclass_1  Pclass_2  Pclass_3\n0      1  0.2750  0.014151         0         0         1\n1      0  0.4750  0.139136         1         0         0\n2      0  0.3250  0.015469         0         0         1\n3      0  0.4375  0.103644         1         0         0\n4      1  0.4375  0.015713         0         0         1\n..   ...     ...       ...       ...       ...       ...\n787    1  0.1000  0.056848         0         0         1\n788    1  0.0125  0.040160         0         0         1\n789    1  0.5750  0.154588         1         0         0\n790    1  0.3500  0.015127         0         0         1\n791    1  0.2000  0.050749         0         1         0\n\n[792 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>Pclass_1</th>\n      <th>Pclass_2</th>\n      <th>Pclass_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0.2750</td>\n      <td>0.014151</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.4750</td>\n      <td>0.139136</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.3250</td>\n      <td>0.015469</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0.4375</td>\n      <td>0.103644</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0.4375</td>\n      <td>0.015713</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>787</th>\n      <td>1</td>\n      <td>0.1000</td>\n      <td>0.056848</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>788</th>\n      <td>1</td>\n      <td>0.0125</td>\n      <td>0.040160</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>789</th>\n      <td>1</td>\n      <td>0.5750</td>\n      <td>0.154588</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>790</th>\n      <td>1</td>\n      <td>0.3500</td>\n      <td>0.015127</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>791</th>\n      <td>1</td>\n      <td>0.2000</td>\n      <td>0.050749</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>792 rows Ã— 6 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y\n","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"0      0\n1      1\n2      1\n3      1\n4      0\n      ..\n787    0\n788    1\n789    0\n790    0\n791    0\nName: Survived, Length: 792, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_t.drop","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<bound method DataFrame.drop of     Survived  Sex     Age      Fare  Pclass_1  Pclass_2  Pclass_3\n0          0    1  0.2000  0.050749         0         1         0\n1          0    0  0.3500  0.135753         0         0         1\n2          0    1  0.3500  0.059914         1         0         0\n3          0    1  0.3125  0.015412         0         0         1\n4          0    1  0.4875  0.025374         0         1         0\n..       ...  ...     ...       ...       ...       ...       ...\n95         0    1  0.3375  0.025374         0         1         0\n96         1    0  0.2375  0.058556         1         0         0\n97         0    0  0.3500  0.045771         0         0         1\n98         1    1  0.3250  0.058556         1         0         0\n99         0    1  0.4000  0.015127         0         0         1\n\n[100 rows x 7 columns]>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(Dense(25,activation='relu'))\nmodel.add(Dense(10,activation='relu'))\nmodel.add(Dense(1))","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='rmsprop',loss='mse')","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_n=X.to_numpy()\ny_n=y.to_numpy()","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=X_n,y=y_n,epochs=250)","execution_count":70,"outputs":[{"output_type":"stream","text":"Train on 792 samples\nEpoch 1/250\n792/792 [==============================] - 0s 483us/sample - loss: 0.3054\nEpoch 2/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.2284\nEpoch 3/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1945\nEpoch 4/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1757\nEpoch 5/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1641\nEpoch 6/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1577\nEpoch 7/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1522\nEpoch 8/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1492\nEpoch 9/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1466\nEpoch 10/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1450\nEpoch 11/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1439\nEpoch 12/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1430\nEpoch 13/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1424\nEpoch 14/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1417\nEpoch 15/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1420\nEpoch 16/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1420\nEpoch 17/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1415\nEpoch 18/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1415\nEpoch 19/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1411\nEpoch 20/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1410\nEpoch 21/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1413\nEpoch 22/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1410\nEpoch 23/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1409\nEpoch 24/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1403\nEpoch 25/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1407\nEpoch 26/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1404\nEpoch 27/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1402\nEpoch 28/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1405\nEpoch 29/250\n792/792 [==============================] - 0s 44us/sample - loss: 0.1399\nEpoch 30/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1400\nEpoch 31/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1400\nEpoch 32/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1399\nEpoch 33/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1396\nEpoch 34/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1396\nEpoch 35/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1393\nEpoch 36/250\n792/792 [==============================] - 0s 42us/sample - loss: 0.1397\nEpoch 37/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1384\nEpoch 38/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1390\nEpoch 39/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1388\nEpoch 40/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1391\nEpoch 41/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1391\nEpoch 42/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1388\nEpoch 43/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1390\nEpoch 44/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1385\nEpoch 45/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1380\nEpoch 46/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1386\nEpoch 47/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1386\nEpoch 48/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1386\nEpoch 49/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1385\nEpoch 50/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1381\nEpoch 51/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1382\nEpoch 52/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1378\nEpoch 53/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1381\nEpoch 54/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1380\nEpoch 55/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1382\nEpoch 56/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1381\nEpoch 57/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1378\nEpoch 58/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1379\nEpoch 59/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1380\nEpoch 60/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1379\nEpoch 61/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1377\nEpoch 62/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1376\nEpoch 63/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1375\nEpoch 64/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1374\nEpoch 65/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1369\nEpoch 66/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1370\nEpoch 67/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1374\nEpoch 68/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1369\nEpoch 69/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1374\nEpoch 70/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1365\nEpoch 71/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1370\nEpoch 72/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1372\nEpoch 73/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1372\nEpoch 74/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1369\nEpoch 75/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1370\nEpoch 76/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1366\nEpoch 77/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1367\nEpoch 78/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1368\nEpoch 79/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1366\nEpoch 80/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1369\nEpoch 81/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1364\nEpoch 82/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1366\nEpoch 83/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1363\nEpoch 84/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1363\nEpoch 85/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1362\nEpoch 86/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1357\nEpoch 87/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1361\nEpoch 88/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1358\nEpoch 89/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1358\nEpoch 90/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1361\nEpoch 91/250\n792/792 [==============================] - 0s 42us/sample - loss: 0.1354\nEpoch 92/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1358\nEpoch 93/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1361\nEpoch 94/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1354\nEpoch 95/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1353\nEpoch 96/250\n","name":"stdout"},{"output_type":"stream","text":"792/792 [==============================] - 0s 41us/sample - loss: 0.1355\nEpoch 97/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1350\nEpoch 98/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1353\nEpoch 99/250\n792/792 [==============================] - 0s 42us/sample - loss: 0.1349\nEpoch 100/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1349\nEpoch 101/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1352\nEpoch 102/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1344\nEpoch 103/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1348\nEpoch 104/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1348\nEpoch 105/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1351\nEpoch 106/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1346\nEpoch 107/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1346\nEpoch 108/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1339\nEpoch 109/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1343\nEpoch 110/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1343\nEpoch 111/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1345\nEpoch 112/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1340\nEpoch 113/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1339\nEpoch 114/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1339\nEpoch 115/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1340\nEpoch 116/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1337\nEpoch 117/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1335\nEpoch 118/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1334\nEpoch 119/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1337\nEpoch 120/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1328\nEpoch 121/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1337\nEpoch 122/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1327\nEpoch 123/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1336\nEpoch 124/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1333\nEpoch 125/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1335\nEpoch 126/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1330\nEpoch 127/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1325\nEpoch 128/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1333\nEpoch 129/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1330\nEpoch 130/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1329\nEpoch 131/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1325\nEpoch 132/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1328\nEpoch 133/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1324\nEpoch 134/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1326\nEpoch 135/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1325\nEpoch 136/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1324\nEpoch 137/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1318\nEpoch 138/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1323\nEpoch 139/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1320\nEpoch 140/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1320\nEpoch 141/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1315\nEpoch 142/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1318\nEpoch 143/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1317\nEpoch 144/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1319\nEpoch 145/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1318\nEpoch 146/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1314\nEpoch 147/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1315\nEpoch 148/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1313\nEpoch 149/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1319\nEpoch 150/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1314\nEpoch 151/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1314\nEpoch 152/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1307\nEpoch 153/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1315\nEpoch 154/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1307\nEpoch 155/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1312\nEpoch 156/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1312\nEpoch 157/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1311\nEpoch 158/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1306\nEpoch 159/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1312\nEpoch 160/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1311\nEpoch 161/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1310\nEpoch 162/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1311\nEpoch 163/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1303\nEpoch 164/250\n792/792 [==============================] - 0s 45us/sample - loss: 0.1307\nEpoch 165/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1309\nEpoch 166/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1304\nEpoch 167/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1308\nEpoch 168/250\n792/792 [==============================] - 0s 42us/sample - loss: 0.1306\nEpoch 169/250\n792/792 [==============================] - 0s 42us/sample - loss: 0.1300\nEpoch 170/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1305\nEpoch 171/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1305\nEpoch 172/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1302\nEpoch 173/250\n792/792 [==============================] - 0s 42us/sample - loss: 0.1307\nEpoch 174/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1302\nEpoch 175/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1303\nEpoch 176/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1302\nEpoch 177/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1301\nEpoch 178/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1296\nEpoch 179/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1299\nEpoch 180/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1293\nEpoch 181/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1297\nEpoch 182/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1299\nEpoch 183/250\n792/792 [==============================] - 0s 45us/sample - loss: 0.1300\nEpoch 184/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1290\nEpoch 185/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1301\nEpoch 186/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1298\nEpoch 187/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1293\nEpoch 188/250\n792/792 [==============================] - 0s 75us/sample - loss: 0.1295\nEpoch 189/250\n792/792 [==============================] - 0s 49us/sample - loss: 0.1299\nEpoch 190/250\n","name":"stdout"},{"output_type":"stream","text":"792/792 [==============================] - 0s 49us/sample - loss: 0.1289\nEpoch 191/250\n792/792 [==============================] - 0s 45us/sample - loss: 0.1297\nEpoch 192/250\n792/792 [==============================] - 0s 49us/sample - loss: 0.1296\nEpoch 193/250\n792/792 [==============================] - 0s 45us/sample - loss: 0.1290\nEpoch 194/250\n792/792 [==============================] - 0s 51us/sample - loss: 0.1289\nEpoch 195/250\n792/792 [==============================] - 0s 46us/sample - loss: 0.1295\nEpoch 196/250\n792/792 [==============================] - 0s 48us/sample - loss: 0.1291\nEpoch 197/250\n792/792 [==============================] - 0s 51us/sample - loss: 0.1292\nEpoch 198/250\n792/792 [==============================] - 0s 45us/sample - loss: 0.1286\nEpoch 199/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1292\nEpoch 200/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1286\nEpoch 201/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1289\nEpoch 202/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1287\nEpoch 203/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1288\nEpoch 204/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1289\nEpoch 205/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1285\nEpoch 206/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1279\nEpoch 207/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1288\nEpoch 208/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1285\nEpoch 209/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1285\nEpoch 210/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1280\nEpoch 211/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1286\nEpoch 212/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1282\nEpoch 213/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1288\nEpoch 214/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1286\nEpoch 215/250\n792/792 [==============================] - 0s 42us/sample - loss: 0.1282\nEpoch 216/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1283\nEpoch 217/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1287\nEpoch 218/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1278\nEpoch 219/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1279\nEpoch 220/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1282\nEpoch 221/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1283\nEpoch 222/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1288\nEpoch 223/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1283\nEpoch 224/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1283\nEpoch 225/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1278\nEpoch 226/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1288\nEpoch 227/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1281\nEpoch 228/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1277\nEpoch 229/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1279\nEpoch 230/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1277\nEpoch 231/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1282\nEpoch 232/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1273\nEpoch 233/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1274\nEpoch 234/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1279\nEpoch 235/250\n792/792 [==============================] - 0s 38us/sample - loss: 0.1283\nEpoch 236/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1281\nEpoch 237/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1279\nEpoch 238/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1281\nEpoch 239/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1281\nEpoch 240/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1275\nEpoch 241/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1276\nEpoch 242/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1278\nEpoch 243/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1278\nEpoch 244/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1273\nEpoch 245/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1273\nEpoch 246/250\n792/792 [==============================] - 0s 53us/sample - loss: 0.1282\nEpoch 247/250\n792/792 [==============================] - 0s 44us/sample - loss: 0.1276\nEpoch 248/250\n792/792 [==============================] - 0s 41us/sample - loss: 0.1272\nEpoch 249/250\n792/792 [==============================] - 0s 39us/sample - loss: 0.1276\nEpoch 250/250\n792/792 [==============================] - 0s 40us/sample - loss: 0.1276\n","name":"stdout"},{"output_type":"execute_result","execution_count":70,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f3255804190>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.history.history","execution_count":71,"outputs":[{"output_type":"execute_result","execution_count":71,"data":{"text/plain":"{'loss': [0.30541268231892826,\n  0.2283701618211438,\n  0.194539703955554,\n  0.1756772988974446,\n  0.16408062989663597,\n  0.1577389020510394,\n  0.15221585754794303,\n  0.14924548987788383,\n  0.14661295922717663,\n  0.14499791312699367,\n  0.14387896852661866,\n  0.1429842555462712,\n  0.1423874076719236,\n  0.1417181146415797,\n  0.14202593041188788,\n  0.14195168244116235,\n  0.14151370164119836,\n  0.14153702180794994,\n  0.14113180948929352,\n  0.14101295308633285,\n  0.14130718689976315,\n  0.14103900295014332,\n  0.1409132160020597,\n  0.1402789791575586,\n  0.14066986528911976,\n  0.14037761859821551,\n  0.1402480256075811,\n  0.14051627006494638,\n  0.13992960148989553,\n  0.14001154041651523,\n  0.14003670125296622,\n  0.13993849898829605,\n  0.13957189083701432,\n  0.13955800445994945,\n  0.1392930807037787,\n  0.13966598218739634,\n  0.13841357465946313,\n  0.13903487150115196,\n  0.13881950200808169,\n  0.13913942572444377,\n  0.13905623592812605,\n  0.1387663068193378,\n  0.13904373302604212,\n  0.13849218167138821,\n  0.13803734065908374,\n  0.13862312073358382,\n  0.13861797911801724,\n  0.13861578936227645,\n  0.13847841728817334,\n  0.1381105524722976,\n  0.1381921682393912,\n  0.13781560212373734,\n  0.1381358468171322,\n  0.13802409924642003,\n  0.1382098003770366,\n  0.13809786541293365,\n  0.13784147588291554,\n  0.13792314583604987,\n  0.1380106323596203,\n  0.13789395942832483,\n  0.1376781067763916,\n  0.13756936246698553,\n  0.13751870739941646,\n  0.13742776154869735,\n  0.13686151817591505,\n  0.13702220221360525,\n  0.1374393237961663,\n  0.13687637794499447,\n  0.1374329301443967,\n  0.13654541141457027,\n  0.13699304259786702,\n  0.13720403461143224,\n  0.13717990346027142,\n  0.13688650659539484,\n  0.13703322199860005,\n  0.13658540011054338,\n  0.13670593545292364,\n  0.13675488132720043,\n  0.13662024308936765,\n  0.13686509279891698,\n  0.13643132812447017,\n  0.13664149462875694,\n  0.13626494025341188,\n  0.13628689478142092,\n  0.13621307864333643,\n  0.13569040972777088,\n  0.13606076868194522,\n  0.135777086019516,\n  0.13582631862825817,\n  0.13612808204359478,\n  0.13541320861891062,\n  0.13575381815734536,\n  0.13614803537575884,\n  0.13535494364873327,\n  0.13527583774894175,\n  0.13547204984257918,\n  0.1349538451794422,\n  0.135266842339376,\n  0.13488844127366037,\n  0.1349013845125834,\n  0.13516983337173558,\n  0.1344342360442335,\n  0.13482863162503098,\n  0.13480670230858255,\n  0.13512266796044628,\n  0.1346003608119608,\n  0.13461342574370028,\n  0.1339371282644946,\n  0.13430888815359635,\n  0.13428239817872192,\n  0.13449625851530017,\n  0.13402438946444578,\n  0.13392484602000979,\n  0.13394882906265934,\n  0.13396237233672478,\n  0.13370762795510918,\n  0.13351038248852046,\n  0.13335052719621948,\n  0.13370683939770014,\n  0.13284835640830223,\n  0.1336720905671216,\n  0.13269565698474345,\n  0.13357317282093895,\n  0.1332736559437983,\n  0.13351046030569558,\n  0.13301500565174854,\n  0.13251014791353785,\n  0.13333717981974283,\n  0.13298764870022284,\n  0.13293391112426314,\n  0.13253313476088072,\n  0.13277716520759794,\n  0.1324099096083882,\n  0.1326255054786952,\n  0.1325075439732484,\n  0.13241967664222526,\n  0.13175602573336978,\n  0.13234524978230697,\n  0.13199915864852943,\n  0.1320294782218307,\n  0.13146271059910455,\n  0.1317746571519158,\n  0.13171556456522507,\n  0.1318903092031527,\n  0.13176283496196825,\n  0.13138980727003077,\n  0.13146721985605028,\n  0.13132565566385634,\n  0.13192063261463186,\n  0.13136930251964415,\n  0.13136173875042886,\n  0.13067550016473037,\n  0.131484830244021,\n  0.13065614799658457,\n  0.13121643850598672,\n  0.13121203340665258,\n  0.1310646454192171,\n  0.1306211030242419,\n  0.13116157069952802,\n  0.13110991853355158,\n  0.13101862732208136,\n  0.13111262708300292,\n  0.13034597490773056,\n  0.13073080943690407,\n  0.13090552698181132,\n  0.13044521649076482,\n  0.13077314785032562,\n  0.13057140659804295,\n  0.1300220222334669,\n  0.1305230383319084,\n  0.13047105736202663,\n  0.13018165822281982,\n  0.13071741043317198,\n  0.1301924092601044,\n  0.13028650023419447,\n  0.1301601262706699,\n  0.13009065058496264,\n  0.12957976969203563,\n  0.12987038553363145,\n  0.12933132187886673,\n  0.12969402735582505,\n  0.129864402115345,\n  0.12998384771623997,\n  0.12903111059256275,\n  0.13013803198783083,\n  0.1297862246030509,\n  0.1293491499893593,\n  0.1295066223000035,\n  0.12986952156731577,\n  0.1289439256293605,\n  0.12965372528391655,\n  0.12963199698262745,\n  0.12900728468943123,\n  0.128868814219128,\n  0.1295221056601014,\n  0.12908147632925196,\n  0.1292066475508189,\n  0.12862833657047965,\n  0.12915862325315525,\n  0.12856865741989829,\n  0.1288686737869725,\n  0.12871746131868073,\n  0.12877837628728211,\n  0.1288837438656224,\n  0.12845961418416765,\n  0.12793314020441035,\n  0.12883353210759885,\n  0.1285084377635609,\n  0.12852900241962587,\n  0.12803915218271392,\n  0.12855854788512894,\n  0.12821030240468304,\n  0.12876483918440462,\n  0.12855201523111323,\n  0.12815941990625979,\n  0.12827210715322784,\n  0.12868560113088048,\n  0.12782804713104712,\n  0.12790424700337227,\n  0.12824369741208624,\n  0.12826321676674515,\n  0.12876461024838265,\n  0.128255130666675,\n  0.1283071706391344,\n  0.12782317098945079,\n  0.12881206246939572,\n  0.12805959567277117,\n  0.12774561240215493,\n  0.1279336576058407,\n  0.1276772865141281,\n  0.1282250327293319,\n  0.12731181943055356,\n  0.12737457875651542,\n  0.12788491507973335,\n  0.12834913848024426,\n  0.12811639190021187,\n  0.12786071544343774,\n  0.12814983695444435,\n  0.1280891141205123,\n  0.12750160927423324,\n  0.12760712469768043,\n  0.12775367137157556,\n  0.127787092869932,\n  0.12733232418094018,\n  0.12728873030705887,\n  0.12822207620348594,\n  0.1276133639944924,\n  0.1272049957528861,\n  0.1275775930044627,\n  0.1275726698263727]}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred=model.predict(X_t)","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fun(x):\n    \n    if x<0.5:\n        x=0\n    else:\n        x=1","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred[pred > 0.5 ] = 1\npred[pred<0.5]=0","execution_count":74,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":75,"outputs":[{"output_type":"execute_result","execution_count":75,"data":{"text/plain":"array([[0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [1.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [1.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [1.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [1.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.],\n       [0.],\n       [1.],\n       [0.],\n       [0.],\n       [0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_t","execution_count":76,"outputs":[{"output_type":"execute_result","execution_count":76,"data":{"text/plain":"0     0\n1     0\n2     0\n3     0\n4     0\n     ..\n95    0\n96    1\n97    0\n98    1\n99    0\nName: Survived, Length: 100, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classification_report(pred,y_t)","execution_count":79,"outputs":[{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"'              precision    recall  f1-score   support\\n\\n         0.0       0.92      0.84      0.88        70\\n         1.0       0.69      0.83      0.76        30\\n\\n    accuracy                           0.84       100\\n   macro avg       0.81      0.84      0.82       100\\nweighted avg       0.85      0.84      0.84       100\\n'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}